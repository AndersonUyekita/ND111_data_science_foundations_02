{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Project - Decision Tree\n",
    "\n",
    "##### Student Tags\n",
    "\n",
    "Author: Anderson Hitoshi Uyekita    \n",
    "Mini-Project: Support Vector Machine  \n",
    "Course: Data Science - Foundations II  \n",
    "COD: ND111  \n",
    "Date: 16/01/2019    \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Introduction](#intro)\n",
    "- [Given code](#code)\n",
    "    - [Function](#function)\n",
    "- [Part 1](#part_1)\n",
    "- [Part 2](#part_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Information\n",
    "\n",
    "This Jupyter Notebook (in Python 2) aims to record all exercise coded to the Support Vector Machine Mini Project.\n",
    "\n",
    "## Introduction <a id='intro'></a>\n",
    "\n",
    "In this project, we will again try to identify the authors in a body of emails, this time using a decision tree. The starter code is in decision_tree/dt_author_id.py.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "#### Decision Tree Mini Project\n",
    "\n",
    "In this project, we will again try to classify emails, this time using a decision tree. The starter code is in `decision_tree/dt_author_id.py`.\n",
    "\n",
    "#### Part 1: Get the Decision Tree Running\n",
    "Get the decision tree up and running as a classifier, setting `min_samples_split=40`.  It will probably take a while to train.  What’s the accuracy?\n",
    "\n",
    "#### Part 2: Speed It Up\n",
    "You found in the SVM mini-project that the parameter tune can significantly speed up the training time of a machine learning algorithm.  A general rule is that the parameters can tune the complexity of the algorithm, with more complex algorithms generally running more slowly.  \n",
    "\n",
    "Another way to control the complexity of an algorithm is via the number of features that you use in training/testing.  The more features the algorithm has available, the more potential there is for a complex fit.  We will explore this in detail in the “Feature Selection” lesson, but you’ll get a sneak preview now.\n",
    "\n",
    "* find the number of features in your data.  The data is organized into a numpy array where the number of rows is the number of data points and the number of columns is the number of features; so to extract this number, use a line of code like `len(features_train[0]`)\n",
    "* go into `tools/email_preprocess.py`, and find the line of code that looks like this:\n",
    "\n",
    "$$selector = SelectPercentile(\\text{f_classif}, percentile=1)$$\n",
    "\n",
    "Change percentile from 10 to 1.\n",
    "\n",
    "* What’s the number of features now?\n",
    "* What do you think SelectPercentile is doing?  Would a large value for percentile lead to a more complex or less complex decision tree, all other things being equal?\n",
    "* Note the difference in training time depending on the number of features.  \n",
    "* What’s the accuracy when `percentile = 1`?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given Code <a id='code'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of Chris training emails: 7936\n",
      "no. of Sara training emails: 7884\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\" \n",
    "    This is the code to accompany the Lesson 2 (SVM) mini-project.\n",
    "\n",
    "    Use a SVM to identify emails from the Enron corpus by their authors:    \n",
    "    Sara has label 0\n",
    "    Chris has label 1\n",
    "\"\"\"\n",
    "    \n",
    "import sys\n",
    "from time import time\n",
    "sys.path.append(\"../tools/\")\n",
    "from email_preprocess import preprocess\n",
    "from email_preprocess_dt import preprocess_dt\n",
    "\n",
    "\n",
    "### features_train and features_test are the features for the training\n",
    "### and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "features_train, features_test, labels_train, labels_test = preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages.\n",
    "\n",
    "# Importing the Scikit Learn package of Support Vector Machine\n",
    "from sklearn import tree\n",
    "\n",
    "# Importing the Scikit Learn to calcutate the accuracy.\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a function <a id='function'></a>\n",
    "\n",
    "This function aims to save lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy.\n",
    "def my_dt(min_samples_split, features_train = features_train, labels_train = labels_train,\n",
    "                   features_test = features_test, labels_test = labels_test, prediction = False):\n",
    "    \"\"\"\n",
    "    This function will calculate the accuracy given the training and test inputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Creating the classifier using the linear kernel.\n",
    "    clf = tree.DecisionTreeClassifier(min_samples_split = 50)\n",
    "\n",
    "    # Saving time to compute the elapse time of fitting process.\n",
    "    t0 = time()\n",
    "\n",
    "    # Fitting/Training clf based on training dataframes.\n",
    "    clf.fit(features_train, labels_train)\n",
    "\n",
    "    # Calculating the elapse time of fit calculation.\n",
    "    print \"training time:\", round(time()-t0, 3), \"s\"\n",
    "\n",
    "    # Saving time to compute the elapse time of predicting process. \n",
    "    t1 = time()\n",
    "\n",
    "    # Storing the predict from features_test in pred.\n",
    "    pred = clf.predict(features_test)\n",
    "\n",
    "    # Calculating the elapse time of predicting calculation.\n",
    "    print \"predict time:\", round(time()-t1, 3), \"s\"\n",
    "\n",
    "    # Calculating the accuracy and storing in acc.\n",
    "    acc = accuracy_score(pred, labels_test)\n",
    "\n",
    "    # Printing the acc.\n",
    "    print \"Accuracy:\", round(acc,4)\n",
    "    \n",
    "    # Returning or not\n",
    "    if prediction == True:\n",
    "        return pred\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 <a id='part_1'></a>\n",
    "\n",
    "Get the decision tree up and running as a classifier, setting `min_samples_split=40`.  It will probably take a while to train.  What’s the accuracy?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 45.212 s\n",
      "predict time: 0.05 s\n",
      "Accuracy: 0.9721\n"
     ]
    }
   ],
   "source": [
    "my_dt(min_samples_split = 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">What is the accuracy of your decision tree?\n",
    "\n",
    "Accuracy: 0.9733"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 <a id='part_2'></a>\n",
    "\n",
    "You found in the SVM mini-project that the parameter tune can significantly speed up the training time of a machine learning algorithm. A general rule is that the parameters can tune the complexity of the algorithm, with more complex algorithms generally running more slowly.\n",
    "\n",
    "Another way to control the complexity of an algorithm is via the number of features that you use in training/testing. The more features the algorithm has available, the more potential there is for a complex fit. We will explore this in detail in the “Feature Selection” lesson, but you’ll get a sneak preview now.\n",
    "\n",
    "**What's the number of features in your data?** (Hint: the data is organized into a numpy array where the number of rows is the number of data points and the number of columns is the number of features; so to extract this number, use a line of code like len(features_train[0]).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3785"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">How many features are in the data?\n",
    "\n",
    "3785.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "go into ../tools/email_preprocess.py, and find the line of code that looks like this:\n",
    "\n",
    "selector = SelectPercentile(f_classif, percentile=10)\n",
    "\n",
    "Change percentile from 10 to 1, and rerun dt_author_id.py. **What’s the number of features now?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of Chris training emails: 7936\n",
      "no. of Sara training emails: 7884\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "379"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I have create an other email_preprocess.py with the change in percentile from 10 to 1.\n",
    "features_train_1, features_test_1, labels_train_1, labels_test_1 = preprocess_dt()\n",
    "\n",
    "# Number of features\n",
    "len(features_train_1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">How many features are in the data now?\n",
    "\n",
    "379.\n",
    "\n",
    "***\n",
    "\n",
    "What do you think SelectPercentile is doing? Would a large value for percentile lead to a more complex or less complex decision tree, all other things being equal? Note the difference in training time depending on the number of features.\n",
    "\n",
    ">Would a large number of features give you a more or less complex decision tree, all other things being equal?\n",
    "\n",
    "more complex\n",
    "\n",
    "***\n",
    "\n",
    "What's the accuracy of your decision tree when you use only 1% of your available features (i.e. percentile=1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 4.14 s\n",
      "predict time: 0.0 s\n",
      "Accuracy: 0.9636\n"
     ]
    }
   ],
   "source": [
    "my_dt(min_samples_split = 40, features_train = features_train_1, labels_train = labels_train_1,\n",
    "                              features_test = features_test_1, labels_test = labels_test_1, prediction = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">What's the accuracy of your decision tree when you use only 1% of your available features (i.e. percentile=1)?\n",
    "\n",
    "Accuracy: 0.9636"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
