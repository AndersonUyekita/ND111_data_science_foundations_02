{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Project - Evaluation Metrics\n",
    "\n",
    "##### Student Tags\n",
    "\n",
    "Author: Anderson Hitoshi Uyekita    \n",
    "Mini-Project: Evaluation Metrics\n",
    "Course: Data Science - Foundations II  \n",
    "COD: ND111  \n",
    "Date: 24/01/2019    \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Introduction](#intro)\n",
    "- [Exercise 1](#part_i_1)\n",
    "- [Exercise 2](#part_i_2)\n",
    "- [Exercise 3](#part_i_3)\n",
    "- [Exercise 4](#part_i_4)\n",
    "- [Exercise 5](#part_i_5)\n",
    "- [Exercise 6](#part_i_6)\n",
    "- [Exercise 7](#part_i_7)\n",
    "- [Exercise 8](#part_i_8)\n",
    "- [Exercise 9](#part_i_9)\n",
    "- [Exercise 10](#part_i_10)\n",
    "- [Exercise 11](#part_i_11)\n",
    "- [Exercise 12](#part_i_12)\n",
    "- [Exercise 13](#part_i_13)\n",
    "- [Exercise 14](#part_i_14)\n",
    "- [Exercise 15](#part_i_15)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries.\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Information\n",
    "\n",
    "This Jupyter Notebook (in Python 2) aims to create a reproducible archive.\n",
    "\n",
    "## Introduction <a id='intro'></a>\n",
    "\n",
    "Go back to your code from the last lesson, where you built a simple first iteration of a POI identifier using a decision tree and one feature. Copy the POI identifier that you built into the skeleton code in evaluation/evaluate_poi_identifier.py. Recall that at the end of that project, your identifier had an accuracy (on the test set) of 0.724. Not too bad, right? Let’s dig into your predictions a little more carefully.  \n",
    "\n",
    "<br>\n",
    "\n",
    "<em>\n",
    "From Python 3.3 forward, a change to the order in which dictionary keys are processed was made such that the orders are randomized each time the code is run. This will cause some compatibility problems with the graders and project code, which were run under Python 2.7. To correct for this, add the following argument to the featureFormat call on line 25 of evaluate_poi_identifier.py:\n",
    "\n",
    "sort_keys = '../tools/python2_lesson14_keys.pkl'\n",
    "\n",
    "This will open up a file in the tools folder with the Python 2 key order.\n",
    "</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - Number of POIs in Test Set <a id='part_i_1'></a>\n",
    "\n",
    ">**How many POIs are predicted for the test set for your POI identifier?**\n",
    "\n",
    "(Note that we said test set! We are not looking for the number of POIs in the whole dataset.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Starter code for the validation mini-project.\n",
    "    The first step toward building your POI identifier!\n",
    "\n",
    "    Start by loading/formatting the data\n",
    "\n",
    "    After that, it's not our code anymore--it's yours!\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "sys.path.append(\"../tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "data_dict = pickle.load(open(\"../final_project/final_project_dataset.pkl\", \"r\") )\n",
    "\n",
    "### first element is our labels, any added elements are predictor\n",
    "### features. Keep this the same for the mini-project, but you'll\n",
    "### have a different feature list when you do the final project.\n",
    "features_list = [\"poi\", \"salary\"]\n",
    "\n",
    "data = featureFormat(data_dict, features_list)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the model selection module.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the dataset into train and test.\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels,test_size=0.30, random_state=42)\n",
    "\n",
    "### it's all yours from here forward!  \n",
    "\n",
    "# Importing the Scikit Learn Decision Tree Module.\n",
    "from sklearn import tree\n",
    "\n",
    "# Creating the Classifier.\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "# Fitting/Training with ALL observations.\n",
    "clf = clf.fit(features_train, labels_train)\n",
    "\n",
    "# Predicting using the same dataset. OVERFITTING!!\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "# Importing numpy.\n",
    "import numpy as np\n",
    "\n",
    "# Importing the Accuracy module.\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculating the accuracy of the overfitted model.\n",
    "acc = accuracy_score(labels_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of POIs: 4.0\n"
     ]
    }
   ],
   "source": [
    "# Printing the number of POI\n",
    "print \"Number of POIs:\", sum(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**How many POIs are predicted for the test set for your POI identifier?**\n",
    "\n",
    "Number of POIs: 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Accuracy of a Biased Identifier <a id='part_i_2'></a>\n",
    "\n",
    ">**If your identifier predicted 0. (not POI) for everyone in the test set, what would its accuracy be?**\n",
    "\n",
    "86.21%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8620689655172413"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming pred equal to a vector of zeros with length of 29 (len(labels_test)).\n",
    "accuracy_score(labels_test, np.zeros((len(labels_test),), dtype=int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - Number of True Positives <a id='part_i_3'></a>\n",
    "\n",
    "Look at the predictions of your model and compare them to the true test labels. Do you get any true positives? (In this case, we define a true positive as a case where both the actual label and the predicted label are 1)\n",
    "\n",
    "- [ ] Yes, many\n",
    "- [ ] Yes, only one\n",
    "- [x] Nope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct</th>\n",
       "      <th>predicted</th>\n",
       "      <th>comparison</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   correct  predicted  comparison\n",
       "0        0          0        True\n",
       "1        0          0        True\n",
       "2        0          0        True\n",
       "3        0          0        True\n",
       "4        0          1       False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a dataframe to work with.\n",
    "comparison = pd.DataFrame(labels_test, columns = ['correct'], dtype = int )\n",
    "\n",
    "# Adding the predicted as a column.\n",
    "comparison['predicted'] = pred.astype(int)\n",
    "\n",
    "# Creating a columns of comparison.\n",
    "comparison['comparison'] = (comparison.correct == comparison.predicted)\n",
    "\n",
    "# Printing the dataframe of comparison.\n",
    "comparison.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of True Positives:  0\n"
     ]
    }
   ],
   "source": [
    "# True positive = correct and predict are equal to 1.\n",
    "true_positive = comparison[(comparison.correct == 1) & (comparison.predicted == 1)]\n",
    "\n",
    "# Number of rows/observations.\n",
    "print \"Number of True Positives: \", true_positive.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Does your identifier have any true positives?**\n",
    "\n",
    "- [ ] Yes, many\n",
    "- [ ] Yes, only one\n",
    "- [x] Nope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - Unpacking Into Precision and Recall<a id='part_i_4'></a>\n",
    "\n",
    "As you may now see, having imbalanced classes like we have in the Enron dataset (many more non-POIs than POIs) introduces some special challenges, namely that you can just guess the more common class label for every point, not a very insightful strategy, and still get pretty good accuracy!\n",
    "\n",
    "Precision and recall can help illuminate your performance better. Use the precision_score and recall_score available in sklearn.metrics to compute those quantities.\n",
    "\n",
    "What’s the precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the metrics module from Scikit Learn package to calculate precision_score.\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Using the function to calculate the precision.\n",
    "precision_score(labels_test, pred)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**What is the precision of your POI identifier?**\n",
    "\n",
    "0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 - Recall of Your POI Identifier <a id='part_i_5'></a>\n",
    "\n",
    "What’s the recall? \n",
    "\n",
    "(Note: you may see a message like UserWarning: The precision and recall are equal to zero for some labels. Just like the message says, there can be problems in computing other metrics (like the F1 score) when precision and/or recall are zero, and it wants to warn you when that happens.) \n",
    "\n",
    "Obviously this isn’t a very optimized machine learning strategy (we haven’t tried any algorithms besides the decision tree, or tuned any parameters, or done any feature selection), and now seeing the precision and recall should make that much more apparent than the accuracy did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the metrics module from Scikit Learn package to calculate recall score.\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Using the function to calculate the precision.\n",
    "recall_score(labels_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**What is the recall of your POI identifier?**\n",
    "\n",
    "0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 - How Many True Positives? <a id='part_i_6'></a>\n",
    "\n",
    "In the final project you’ll work on optimizing your POI identifier, using many of the tools learned in this course. Hopefully one result will be that your precision and/or recall will go up, but then you’ll have to be able to interpret them. \n",
    "\n",
    "Here are some made-up predictions and true labels for a hypothetical test set; fill in the following boxes to practice identifying true positives, false positives, true negatives, and false negatives. Let’s use the convention that “1” signifies a positive result, and “0” a negative. \n",
    "\n",
    "predictions = [0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1]   \n",
    "true labels = [0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0]  \n",
    "\n",
    "How many true positives are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to analize the True Positives.\n",
    "def is_true_positive(predictions, true_labels):\n",
    "    \"\"\"\n",
    "    Returns the number of occurencies of true positive.\n",
    "    Both inputs are predictions and true_labels.\n",
    "    \"\"\"\n",
    "    # Creating a dataframe.\n",
    "    predictions = pd.DataFrame(predictions, columns = ['predictions'])\n",
    "    predictions['true_labels'] = true_labels\n",
    "\n",
    "    # True Positives\n",
    "    true_positive = predictions[(predictions.predictions == 1) & (predictions.true_labels == 1)]\n",
    "    \n",
    "    return true_positive.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of True Positives: 6\n"
     ]
    }
   ],
   "source": [
    "# Given results.\n",
    "predictions = [0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1] \n",
    "true_labels = [0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0]\n",
    "\n",
    "# Using the given inputs.\n",
    "print \"Number of True Positives:\", is_true_positive(predictions, true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**How many true positives are there?**\n",
    "\n",
    "Number of True Positives: 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7 - How Many True Negatives? <a id='part_i_7'></a>\n",
    "\n",
    "Suppose our data looks like this:\n",
    "\n",
    "predictions = [0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1]   \n",
    "true labels = [0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0]\n",
    "\n",
    "(this is fabricated data, just to give you some practice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of True Negatives: 9\n"
     ]
    }
   ],
   "source": [
    "# Given results.\n",
    "predictions = [0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1]\n",
    "true_labels = [0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0]\n",
    "\n",
    "predictions = pd.DataFrame(predictions, columns = ['predictions'])\n",
    "predictions['true_labels'] = true_labels\n",
    "\n",
    "# True Negative = true label and predictions equal to zero.\n",
    "true_negative = predictions.query('true_labels == \"0\"').query('predictions == \"0\"')\n",
    "\n",
    "# Using the given inputs.\n",
    "print \"Number of True Negatives:\", true_negative.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**How many true negatives are there in this example?**\n",
    "\n",
    "Number of True Negatives: 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8 - False Positives? <a id='part_i_8'></a>\n",
    "\n",
    ">**How many false positives are there?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of False Positives: 3\n"
     ]
    }
   ],
   "source": [
    "# False Positives = Is truly false but predict as positive.\n",
    "true_negative = predictions.query('true_labels == \"0\"').query('predictions == \"1\"')\n",
    "\n",
    "# Using the given inputs.\n",
    "print \"Number of False Positives:\", true_negative.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9 - False Negatives? <a id='part_i_9'></a>\n",
    "\n",
    ">**How many false negatives are there?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of False Negative: 2\n"
     ]
    }
   ],
   "source": [
    "# False Negatives = Is trully positive but predicted as negative.\n",
    "true_negative = predictions.query('true_labels == \"1\"').query('predictions == \"0\"')\n",
    "\n",
    "# Using the given inputs.\n",
    "print \"Number of False Negative:\", true_negative.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10 - Precision <a id='part_i_10'></a>\n",
    "\n",
    ">**What's the precision of this classifier?**\n",
    "\n",
    "$$\\text{Precision} = \\frac{\\text{True Positive}}{\\text{True Positive + False Positive}} = \\frac{6}{6 + 3} = \\frac{2}{3} = 0.6666$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# Given results.\n",
    "predictions = [0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1]\n",
    "true_labels = [0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0]\n",
    "\n",
    "# Using the function to calculate the precision.\n",
    "print \"Precision: \", precision_score(true_labels, np.array(predictions))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 11 - Recall <a id='part_i_11'></a>\n",
    "\n",
    ">**What's the recall of this classifier?**\n",
    "\n",
    "$$\\text{Recall} = \\frac{\\text{True Positive}}{\\text{True Positive + False Negative}} = \\frac{6}{6+2} = \\frac{3}{4} = 0.75$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  0.75\n"
     ]
    }
   ],
   "source": [
    "# Using the function to calculate the precision.\n",
    "print \"Recall: \", recall_score(true_labels, np.array(predictions))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 12 - Making Sense of Metrics 1 <a id='part_i_12'></a>\n",
    "\n",
    "\n",
    "Fill in the blank:\n",
    "\n",
    "“My true positive rate is high, which means that when a _ _ _ _ is present in the test data, I am good at flagging him or her.”\n",
    "\n",
    "- [x] POI\n",
    "- [ ] non-POI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 13 - Making Sense of Metrics 2 <a id='part_i_13'></a>\n",
    "\n",
    "Fill in the blanks.\n",
    "\n",
    "“My identifier doesn’t have great _ _, but it does have good _ _ _ _. That means that whenever a POI gets flagged in my test set, I know with a lot of confidence that it’s very likely to be a real POI and not a false alarm. On the other hand, the price I pay for this is that I sometimes miss real POIs, since I’m effectively reluctant to pull the trigger on edge cases.”\n",
    "\n",
    "- [ ] recall/precision\n",
    "- [x] precision/recall\n",
    "- [ ] F1 score/recall\n",
    "- [ ] precision/F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 14 - Making Sense of Metrics 3 <a id='part_i_14'></a>\n",
    "\n",
    "“My identifier doesn’t have great _ _, but it does have good _ _ _ _. That means that whenever a POI gets flagged in my test set, I know with a lot of confidence that it’s very likely to be a real POI and not a false alarm. On the other hand, the price I pay for this is that I sometimes miss real POIs, since I’m effectively reluctant to pull the trigger on edge cases.”\n",
    "\n",
    "- [x] recall/precision\n",
    "- [ ] precision/recall\n",
    "- [ ] F1 score/recall\n",
    "- [ ] precision/F1 score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 15 - Making Sense of Metrics 4 <a id='part_i_15'></a>\n",
    "\n",
    "“My identifier has a really great _ _.\n",
    "\n",
    "This is the best of both worlds. Both my false positive and false negative rates are _ _, which means that I can identify POI’s reliably and accurately. If my identifier finds a POI then the person is almost certainly a POI, and if the identifier does not flag someone, then they are almost certainly not a POI.”\n",
    "\n",
    "- [ ] recall/precision\n",
    "- [ ] precision/recall\n",
    "- [x] F1 score/recall\n",
    "- [ ] precision/F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copying file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing shutil to deal with copy\n",
    "from shutil import copyfile\n",
    "\n",
    "# File name\n",
    "filename = 'evaluate_poi_identifier.ipynb'\n",
    "\n",
    "# Lesson\n",
    "lesson = '15-Lesson_15'\n",
    "\n",
    "# Directory to make a copy\n",
    "dir_copy = '../../' + lesson + '/00-Mini Project/' + filename\n",
    "\n",
    "# Copying file.\n",
    "copyfile(filename, dir_copy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
