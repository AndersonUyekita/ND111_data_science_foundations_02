{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Project - Support Vector Machine\n",
    "\n",
    "##### Student Tags\n",
    "\n",
    "Author: Anderson Hitoshi Uyekita    \n",
    "Mini-Project: Support Vector Machine  \n",
    "Course: Data Science - Foundations II  \n",
    "COD: ND111  \n",
    "Date: 15/01/2019    \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Introduction](#intro)\n",
    "- [Given code](#code)\n",
    "    - [Function](#function)\n",
    "- [Exercise 1](#exercise_1)\n",
    "- [Exercise 2](#exercise_2)\n",
    "- [Exercise 3](#exercise_3)\n",
    "- [Exercise 4](#exercise_4)\n",
    "- [Exercise 5](#exercise_5)\n",
    "- [Exercise 6](#exercise_6)\n",
    "- [Exercise 7](#exercise_7)\n",
    "- [Exercise 8](#exercise_8)\n",
    "- [Exercise 9](#exercise_9)\n",
    "- [Exercise 10](#exercise_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Information\n",
    "\n",
    "This Jupyter Notebook (in Python 2) aims to record all exercise coded to the Support Vector Machine Mini Project.\n",
    "\n",
    "## Introduction <a id='intro'></a>\n",
    "\n",
    "In this mini-project, we’ll tackle the exact same email author ID problem as the Naive Bayes mini-project, but now with an SVM. What we find will help clarify some of the practical differences between the two algorithms. This project also gives us a chance to play around with parameters a lot more than Naive Bayes did, so we will do that too.\n",
    "\n",
    "\n",
    "## Given Code <a id='code'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of Chris training emails: 7936\n",
      "no. of Sara training emails: 7884\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "\"\"\" \n",
    "    This is the code to accompany the Lesson 2 (SVM) mini-project.\n",
    "\n",
    "    Use a SVM to identify emails from the Enron corpus by their authors:    \n",
    "    Sara has label 0\n",
    "    Chris has label 1\n",
    "\"\"\"\n",
    "    \n",
    "import sys\n",
    "from time import time\n",
    "sys.path.append(\"../tools/\")\n",
    "from email_preprocess import preprocess\n",
    "\n",
    "\n",
    "### features_train and features_test are the features for the training\n",
    "### and testing datasets, respectively\n",
    "### labels_train and labels_test are the corresponding item labels\n",
    "features_train, features_test, labels_train, labels_test = preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a function <a id='function'></a>\n",
    "\n",
    "This function aims to save lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy.\n",
    "def my_SVC(kernel, features_train = features_train, labels_train = labels_train,\n",
    "                   features_test = features_test, labels_test = labels_test,\n",
    "           C = 0, gamma = 0, prediction = False):\n",
    "    \"\"\"\n",
    "    This function will calculate the accuracy given the kernek, C, and gamma.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Creating the classifier using the linear kernel.\n",
    "    if (C == 0) & (gamma == 0):\n",
    "        clf = SVC(kernel = kernel)\n",
    "    elif gamma == 0:\n",
    "        clf = SVC(kernel=kernel, C = C)\n",
    "    else:\n",
    "        clf = SVC(kernel=kernel, gamma=gamma)\n",
    "        \n",
    "    #clf = SVC(kernel=kernel, C = C, gamma = gamma)\n",
    "\n",
    "    # Saving time to compute the elapse time of fitting process.\n",
    "    t0 = time()\n",
    "\n",
    "    # Fitting/Training clf based on training dataframes.\n",
    "    clf.fit(features_train, labels_train)\n",
    "\n",
    "    # Calculating the elapse time of fit calculation.\n",
    "    print \"training time:\", round(time()-t0, 3), \"s\"\n",
    "\n",
    "    # Saving time to compute the elapse time of predicting process. \n",
    "    t1 = time()\n",
    "\n",
    "    # Storing the predict from features_test in pred.\n",
    "    pred = clf.predict(features_test)\n",
    "\n",
    "    # Calculating the elapse time of predicting calculation.\n",
    "    print \"training time:\", round(time()-t1, 3), \"s\"\n",
    "\n",
    "    # Calculating the accuracy and storing in acc.\n",
    "    acc = accuracy_score(pred, labels_test)\n",
    "\n",
    "    # Printing the acc.\n",
    "    print \"Accuracy:\", round(acc,4)\n",
    "    \n",
    "    # Returning or not\n",
    "    if prediction == True:\n",
    "        return pred\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages.\n",
    "\n",
    "# Importing the Scikit Learn package of Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Importing the Scikit Learn to calcutate the accuracy.\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - SVM Author ID Accuracy <a id='exercise_1'></a>\n",
    "\n",
    "Go to the svm directory to find the starter code (svm/svm_author_id.py).\n",
    "\n",
    "Import, create, train and make predictions with the sklearn SVC classifier. When creating the classifier, use a linear kernel (if you forget this step, you will be unpleasantly surprised by how long the classifier takes to train). What is the accuracy of the classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 176.859 s\n",
      "training time: 17.849 s\n",
      "Accuracy: 0.9841\n"
     ]
    }
   ],
   "source": [
    "my_SVC(kernel = 'linear', C = 0, gamma = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">What is the accuracy of your author identification SVM?\n",
    "\n",
    "Accuracy: 0.9841"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - SVM Author ID Timing <a id='exercise_2'></a>\n",
    "\n",
    "Place timing code around the fit and predict functions, like you did in the Naive Bayes mini-project. How do the training and prediction times compare to Naive Bayes?\n",
    "\n",
    ">Are the SVM training and predicting times faster or slower than Naïve Bayes?\n",
    "\n",
    "Slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - A Smaller Training Set <a id='exercise_3'></a>\n",
    "\n",
    "One way to speed up an algorithm is to train it on a smaller training dataset. The tradeoff is that the accuracy almost always goes down when you do this. Let’s explore this more concretely: add in the following two lines immediately before training your classifier. \n",
    "\n",
    "features_train = features_train[:len(features_train)/100] \n",
    "labels_train = labels_train[:len(labels_train)/100] \n",
    "\n",
    "These lines effectively slice the training dataset down to 1% of its original size, tossing out 99% of the training data. You can leave all other code unchanged. What’s the accuracy now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 0.184 s\n",
      "training time: 1.125 s\n",
      "Accuracy: 0.8845\n"
     ]
    }
   ],
   "source": [
    "my_SVC(kernel = 'linear', features_train = features_train[:len(features_train)/100] ,\n",
    "                          labels_train = labels_train[:len(labels_train)/100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">What's the accuracy of your SVM now?\n",
    "\n",
    "Accuracy: 0.8845"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - Speed-Accuracy Tradeoff <a id='exercise_4'></a>\n",
    "\n",
    "If speed is a major consideration (and for many real-time machine learning applications, it certainly is) then you may want to sacrifice a bit of accuracy if it means you can train/predict faster. Which of these are applications where you can imagine a very quick-running algorithm is especially important?\n",
    "\n",
    "predicting the author of an email\n",
    "flagging credit card fraud, and blocking a transaction before it goes through\n",
    "voice recognition, like Siri\n",
    "\n",
    ">Which of these are applications where you can imagine a very quicky-running algorithm is specially important?\n",
    "\n",
    "* [ ] Predicting the author of an email;\n",
    "* [x] flagging credit card fraud, and blocking a transaction before it goes through;\n",
    "* [x] Voice recognition, like Siri.\n",
    "\n",
    "We agree!  Voice recognition and transaction blocking need to happen in real time, with almost no delay.  There's no obvious need to predict an email author instantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 - Deploy an RBF Kernel <a id='exercise_5'></a>\n",
    "\n",
    "Keep the training set slice code from the last quiz, so that you are still training on only 1% of the full training set. Change the kernel of your SVM to “rbf”. What’s the accuracy now, with this more complex kernel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 0.165 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python27\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 1.176 s\n",
      "Accuracy: 0.616\n"
     ]
    }
   ],
   "source": [
    "my_SVC(kernel = 'rbf', features_train = features_train[:len(features_train)/100] ,\n",
    "                          labels_train = labels_train[:len(labels_train)/100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">What is the accuracy of your SMV now, with this more complex kernel?\n",
    "\n",
    "Accuracy: 0.6160"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 - Optimize C Parameter <a id='exercise_6'></a>\n",
    "\n",
    "Keep the training set size and rbf kernel from the last quiz, but try several values of C (say, 10.0, 100., 1000., and 10000.). Which one gives the best accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 0.151 s\n",
      "training time: 1.158 s\n",
      "Accuracy: 0.616\n",
      "training time: 0.101 s\n",
      "training time: 1.161 s\n",
      "Accuracy: 0.616\n",
      "training time: 0.122 s\n",
      "training time: 1.141 s\n",
      "Accuracy: 0.8214\n",
      "training time: 0.121 s\n",
      "training time: 0.919 s\n",
      "Accuracy: 0.8925\n"
     ]
    }
   ],
   "source": [
    "# List of C values to be tested.\n",
    "list_C = [10, 100, 1000, 10000]\n",
    "\n",
    "# Loop to calculate the accuracy given the C values.\n",
    "for index in list_C:\n",
    "    my_SVC(kernel = 'rbf', features_train = features_train[:len(features_train)/100] ,\n",
    "                          labels_train = labels_train[:len(labels_train)/100], C = index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Which of these values of C gives the best SVM accuracy?\n",
    "\n",
    "For C equal to 10,000 the accuracy is 0.8925."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7 - Accuracy after Optimizing C <a id='exercise_7'></a>\n",
    "\n",
    "Once you've optimized the C value for your RBF kernel, what accuracy does it give? Does this C value correspond to a simpler or more complex decision boundary?\n",
    "\n",
    "(If you're not sure about the complexity, go back a few videos to the \"SVM C Parameter\" part of the lesson. The result that you found there is also applicable here, even though it's now much harder or even impossible to draw the decision boundary in a simple scatterplot.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">What's the accuracy of your SVM now? Is the boundary more or less complex than if C its default values of 1.0?\n",
    "\n",
    "* Accuracy: 0.8925\n",
    "* More complex\n",
    "\n",
    "A low `C` makes the decision surface smooth, while a high `C` aims at classifying all training examples correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8 - Optimized RBF vs. Linear SVM: Accuracy <a id='exercise_8'></a>\n",
    "\n",
    "Now that you’ve optimized C for the RBF kernel, go back to using the full training set. In general, having a larger training set will improve the performance of your algorithm, so (by tuning C and training on a large dataset) we should get a fairly optimized result. What is the accuracy of the optimized SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 121.597 s\n",
      "training time: 12.316 s\n",
      "Accuracy: 0.9909\n"
     ]
    }
   ],
   "source": [
    "my_SVC(kernel = 'rbf', C = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">What's the accuracy of your optimized SVM?\n",
    "\n",
    "Accuracy: 0.9909"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9 - Extracting Predictions from an SVM <a id='exercise_9'></a>\n",
    "\n",
    "What class does your SVM (0 or 1, corresponding to Sara and Chris respectively) predict for element 10 of the test set? The 26th? The 50th? (Use the RBF kernel, C=10000, and 1% of the training set. Normally you'd get the best results using the full training set, but we found that using 1% sped up the computation considerably and did not change our results--so feel free to use that shortcut here.)\n",
    "\n",
    "And just to be clear, the data point numbers that we give here (10, 26, 50) assume a zero-indexed list. So the correct answer for element #100 would be found using something like answer=predictions[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 0.122 s\n",
      "training time: 0.901 s\n",
      "Accuracy: 0.8925\n"
     ]
    }
   ],
   "source": [
    "pred_9 = my_SVC(kernel = 'rbf', features_train = features_train[:len(features_train)/100] ,\n",
    "                                labels_train = labels_train[:len(labels_train)/100], C = 10000, prediction = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results of 10, 26 and 50.\n",
    "pred_9[10], pred_9[26], pred_9[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to be clear, the data point numbers that we give here (10, 26, 50) assume a zero-indexed list. So the correct answer for element #100 would be found using something like answer=predictions[100]\n",
    "\n",
    ">What class does your SVM (0 or 1, corresponding to Sara and Chris respectively) predict for element 10 of the test? The 26th? The 50th?\n",
    "\n",
    "* 10: 1 (Chris);\n",
    "* 26: 0 (Sara), and;\n",
    "* 50: 1 (Chris)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10 - How Many Chris Emails Predicted? <a id='exercise_10'></a>\n",
    "\n",
    "There are over 1700 test events--how many are predicted to be in the “Chris” (1) class? (Use the RBF kernel, C=10000., and the full training set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 113.98 s\n",
      "training time: 10.663 s\n",
      "Accuracy: 0.9909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "877"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_10 = my_SVC(kernel = 'rbf', C = 10000, prediction = True)\n",
    "\n",
    "sum(pred_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">There are over 1700 test events-how many are predicted to be the \"Chris\" (1) class? \n",
    "\n",
    "877 classification as Chris."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
