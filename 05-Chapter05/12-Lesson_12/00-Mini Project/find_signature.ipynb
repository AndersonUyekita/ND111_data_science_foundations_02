{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Project - Feature Selection\n",
    "\n",
    "##### Student Tags\n",
    "\n",
    "Author: Anderson Hitoshi Uyekita    \n",
    "Mini-Project: Feature Selection  \n",
    "Course: Data Science - Foundations II  \n",
    "COD: ND111  \n",
    "Date: 23/01/2019    \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Introduction](#intro)\n",
    "- [Given code 1](#code1)\n",
    "- [Exercise 1](#part_i_1)\n",
    "- [Exercise 2](#part_i_2)\n",
    "- [Exercise 3](#part_i_3)\n",
    "- [Exercise 4](#part_i_4)\n",
    "- [Exercise 5](#part_i_5)\n",
    "- [Exercise 6](#part_i_6)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries.\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Information\n",
    "\n",
    "This Jupyter Notebook (in Python 2) aims to create a reproducible archive.\n",
    "\n",
    "## Introduction <a id='intro'></a>\n",
    "\n",
    "Katie explained in a video a problem that arose in preparing Chris and Sara’s email for the author identification project; it had to do with a feature that was a little too powerful (effectively acting like a signature, which gives an arguably unfair advantage to an algorithm). You’ll work through that discovery process here.\n",
    "\n",
    "## Exercise 1 - Overfitting a Decision Tree 1 <a id='part_i_1'></a>\n",
    "\n",
    "This bug was found when Katie was trying to make an overfit decision tree to use as an example in the decision tree mini-project. A decision tree is classically an algorithm that can be easy to overfit; one of the easiest ways to get an overfit decision tree is to use a small training set and lots of features.\n",
    "\n",
    ">**If a decision tree is overfit, would you expect the accuracy on a test set to be very high or pretty low?**\n",
    "\n",
    "Low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Overfitting a Decision Tree 2 <a id='part_i_2'></a>\n",
    "\n",
    ">**If a decision tree is overfit, would you expect high or low accuracy on the training set?**\n",
    "\n",
    "High.\n",
    "\n",
    "Exactly. The accuracy would be very high on the training set, but would plummet once it was actually tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - Number of Features and Overfitting <a id='part_i_3'></a>\n",
    "\n",
    "A classic way to overfit an algorithm is by using lots of features and not a lot of training data. You can find the starter code in feature_selection/find_signature.py. Get a decision tree up and training on the training data, and print out the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 0.173 s\n",
      "predict time: 0.443 s\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import pickle\n",
    "import numpy\n",
    "numpy.random.seed(42)\n",
    "\n",
    "\n",
    "### The words (features) and authors (labels), already largely processed.\n",
    "### These files should have been created from the previous (Lesson 10)\n",
    "### mini-project.\n",
    "words_file = \"../text_learning/your_word_data.pkl\" \n",
    "authors_file = \"../text_learning/your_email_authors.pkl\"\n",
    "word_data = pickle.load( open(words_file, \"r\"))\n",
    "authors = pickle.load( open(authors_file, \"r\") )\n",
    "\n",
    "\n",
    "\n",
    "### test_size is the percentage of events assigned to the test set (the\n",
    "### remainder go into training)\n",
    "### feature matrices changed to dense representations for compatibility with\n",
    "### classifier functions in versions 0.15.2 and earlier\n",
    "from sklearn import model_selection\n",
    "features_train, features_test, labels_train, labels_test = model_selection.train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "features_train = vectorizer.fit_transform(features_train)\n",
    "features_test  = vectorizer.transform(features_test).toarray()\n",
    "\n",
    "\n",
    "### a classic way to overfit is to use a small number\n",
    "### of data points and a large number of features;\n",
    "### train on only 150 events to put ourselves in this regime\n",
    "features_train = features_train[:150].toarray()\n",
    "labels_train   = labels_train[:150]\n",
    "\n",
    "### your code goes here\n",
    "\n",
    "# Importing the Scikit Learn package of Support Vector Machine\n",
    "from sklearn import tree\n",
    "\n",
    "# Importing the Scikit Learn to calcutate the accuracy.\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Importing time to compute the time elapsed.\n",
    "from time import time\n",
    "\n",
    "# Creating the classifier using the linear kernel.\n",
    "clf = tree.DecisionTreeClassifier(min_samples_split = 50)\n",
    "\n",
    "# Saving time to compute the elapse time of fitting process.\n",
    "t0 = time()\n",
    "\n",
    "# Fitting/Training clf based on training dataframes.\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "# Calculating the elapse time of fit calculation.\n",
    "print \"training time:\", round(time()-t0, 3), \"s\"\n",
    "\n",
    "# Saving time to compute the elapse time of predicting process. \n",
    "t1 = time()\n",
    "\n",
    "# Storing the predict from features_test in pred.\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "# Calculating the elapse time of predicting calculation.\n",
    "print \"predict time:\", round(time()-t1, 3), \"s\"\n",
    "\n",
    "# Calculating the accuracy and storing in acc.\n",
    "acc = accuracy_score(pred, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150L"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of training points.\n",
    "features_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**How many training points are there, according to the starter code?**\n",
    "\n",
    "150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - Accuracy of Your Overfit Decision Tree <a id='part_i_4'></a>\n",
    "\n",
    ">**What’s the accuracy of the decision tree you just made?** (Remember, we're setting up our decision tree to overfit -- ideally, we want to see the test accuracy as relatively low.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9477\n"
     ]
    }
   ],
   "source": [
    "# Printing the acc.\n",
    "print \"Accuracy:\", round(acc,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 - Identify the Most Powerful Features <a id='part_i_5'></a>\n",
    "\n",
    "Take your (overfit) decision tree and use the feature_importances_ attribute to get a list of the relative importance of all the features being used. We suggest iterating through this list (it’s long, since this is text data) and only printing out the feature importance if it’s above some threshold (say, 0.2--remember, if all words were equally important, each one would give an importance of far less than 0.01).\n",
    "\n",
    ">**What’s the importance of the most important feature? What is the number of this feature?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7647058823529412"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importance.\n",
    "max(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33614"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing pandas.\n",
    "import numpy as np\n",
    "\n",
    "# Finding the \"index\" of the features above (0.7647058823529412)\n",
    "np.where(max(clf.feature_importances_) == clf.feature_importances_)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 - Identify the Most Powerful Features <a id='part_i_6'></a>\n",
    "\n",
    "In order to figure out what words are causing the problem, you need to go back to the TfIdf and use the feature numbers that you obtained in the previous part of the mini-project to get the associated words. You can return a list of all the words in the TfIdf by calling get_feature_names() on it; pull out the word that’s causing most of the discrimination of the decision tree. What is it? Does it make sense as a word that’s uniquely tied to either Chris Germany or Sara Shackleton, a signature of sorts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sshacklensf'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the word of 33614 index?\n",
    "vectorizer.get_feature_names()[33614].encode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**What is the most powerful word when your decision tree is making its classification decisions?**\n",
    "\n",
    "sshacklensf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7 - Remove, Repeat <a id='part_i_7'></a>\n",
    "\n",
    "This word seems like an outlier in a certain sense, so let’s remove it and refit. Go back to text_learning/vectorize_text.py, and remove this word from the emails using the same method you used to remove “sara”, “chris”, etc. Rerun vectorize_text.py, and once that finishes, rerun find_signature.py. Any other outliers pop up? What word is it? Seem like a signature-type word? (Define an outlier as a feature with importance >0.2, as before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi everyon if you can read this messag your proper use parseouttext pleas proceed to the next part of the project\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "\n",
    "def parseOutText_v2(f):\n",
    "    \"\"\" given an opened email file f, parse out all text below the\n",
    "        metadata block at the top\n",
    "        (in Part 2, you will also add stemming capabilities)\n",
    "        and return a string that contains all the words\n",
    "        in the email (space-separated) \n",
    "        \n",
    "        example use case:\n",
    "        f = open(\"email_file_name.txt\", \"r\")\n",
    "        text = parseOutText(f)\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    f.seek(0)  ### go back to beginning of file (annoying)\n",
    "    all_text = f.read()\n",
    "\n",
    "    ### split off metadata\n",
    "    content = all_text.split(\"X-FileName:\")\n",
    "    words = \"\"\n",
    "    if len(content) > 1:\n",
    "        ### remove punctuation\n",
    "        text_string = content[1].translate(string.maketrans(\"\", \"\"), string.punctuation)\n",
    "\n",
    "        ### project part 2: comment out the line below\n",
    "        words = text_string\n",
    "\n",
    "        ### split the text string into individual words, stem each word,\n",
    "        ### and append the stemmed word to words (make sure there's a single\n",
    "        ### space between each stemmed word)\n",
    "        \n",
    "        # Converting \\n in spaces\n",
    "        words = words.replace('\\n',' ')\n",
    "\n",
    "        # Removing punctuation\n",
    "        # https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python\n",
    "        words = words.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "        \n",
    "        # Importing re package\n",
    "        import re\n",
    "        \n",
    "        # Removing any instance of double or more spaces.\n",
    "        words = re.sub(' +', ' ',words)\n",
    "        \n",
    "        # Removing spaces in the begining and ending of the email.\n",
    "        words = words.lstrip().rstrip()\n",
    "        \n",
    "        # Splitting by space. Creating a vector of words.\n",
    "        words = words.split()\n",
    "        \n",
    "        # Creating the Classifier using english as language.\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        \n",
    "        # Stemming each word of the vector.\n",
    "        words = map(lambda x : stemmer.stem(x), words);\n",
    "        \n",
    "        # Binding all words together to became a string again.\n",
    "        words = ' '.join(words)\n",
    "            \n",
    "    # Converting to UTF-8\n",
    "    return words.encode(\"utf-8\")\n",
    "\n",
    "def main():\n",
    "    ff = open(\"../text_learning/test_email.txt\", \"r\")\n",
    "    text = parseOutText_v2(ff)\n",
    "    print text\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emails processed\n",
      "Processing time:  1.85 minutes\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Starter code to process the emails from Sara and Chris to extract\n",
    "    the features and get the documents ready for classification.\n",
    "\n",
    "    The list of all the emails from Sara are in the from_sara list\n",
    "    likewise for emails from Chris (from_chris)\n",
    "\n",
    "    The actual documents are in the Enron email dataset, which\n",
    "    you downloaded/unpacked in Part 0 of the first mini-project. If you have\n",
    "    not obtained the Enron email corpus, run startup.py in the tools folder.\n",
    "\n",
    "    The data is stored in lists and packed away in pickle files at the end.\n",
    "\"\"\"\n",
    "\n",
    "from_sara  = open(\"../text_learning/from_sara.txt\", \"r\")\n",
    "from_chris = open(\"../text_learning/from_chris.txt\", \"r\")\n",
    "\n",
    "from_data = []\n",
    "word_data = []\n",
    "\n",
    "### temp_counter is a way to speed up the development--there are\n",
    "### thousands of emails from Sara and Chris, so running over all of them\n",
    "### can take a long time\n",
    "### temp_counter helps you only look at the first 200 emails in the list so you\n",
    "### can iterate your modifications quicker\n",
    "temp_counter = 0\n",
    "\n",
    "# Start\n",
    "start = time.time()\n",
    "\n",
    "for name, from_person in [(\"sara\", from_sara), (\"chris\", from_chris)]:\n",
    "    for path in from_person:\n",
    "        ### only look at first 200 emails when developing\n",
    "        ### once everything is working, remove this line to run over full dataset\n",
    "        temp_counter += 1\n",
    "        if temp_counter > -1:\n",
    "            path = os.path.join('..', path[:-1])\n",
    "            #print path\n",
    "            email = open(path, \"r\")\n",
    "\n",
    "            ### use parseOutText to extract the text from the opened email\n",
    "            parsed_email = parseOutText_v2(email)\n",
    "                \n",
    "            ### use str.replace() to remove any instances of the words\n",
    "            ### [\"sara\", \"shackleton\", \"chris\", \"germani\"]\n",
    "            parsed_email_clean = parsed_email.replace('sara','').replace('chris','').replace('shackleton','').replace('germani','')\n",
    "            \n",
    "            parsed_email_clean = parsed_email_clean.replace('sshacklensf','')\n",
    "            \n",
    "            ### append the text to word_data\n",
    "            word_data.append(parsed_email_clean)\n",
    "            \n",
    "            ### append a 0 to from_data if email is from Sara, and 1 if email is from Chris\n",
    "            if name == 'sara':\n",
    "                from_data.append(0)\n",
    "            elif name == 'chris':\n",
    "                from_data.append(1)\n",
    "            else:\n",
    "                print \"ERROR!!\"\n",
    "            \n",
    "            email.close()\n",
    "\n",
    "print \"emails processed\"\n",
    "from_sara.close()\n",
    "from_chris.close()\n",
    "\n",
    "pickle.dump( word_data, open(\"your_word_data_fs.pkl\", \"w\") )\n",
    "pickle.dump( from_data, open(\"your_email_authors_fs.pkl\", \"w\") )\n",
    "\n",
    "# Stop\n",
    "stop = time.time()\n",
    "\n",
    "print \"Processing time: \", round((stop - start)/60,2), \"minutes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38756"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### in Part 4, do TfIdf vectorization here\n",
    "\n",
    "# Importing the TfIdf package.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Creating the vectorizer using english stopwords.\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english', lowercase=True)\n",
    "\n",
    "# Fitting and Transforming.\n",
    "vectorizer.fit_transform(word_data)\n",
    "\n",
    "# Creating the Vocabulary List.\n",
    "vocab_list = vectorizer.get_feature_names()\n",
    "\n",
    "# Length of the Vocabulary.\n",
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 0.139 s\n",
      "predict time: 0.48 s\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import pickle\n",
    "import numpy\n",
    "numpy.random.seed(42)\n",
    "\n",
    "\n",
    "### The words (features) and authors (labels), already largely processed.\n",
    "### These files should have been created from the previous (Lesson 10)\n",
    "### mini-project.\n",
    "words_file = \"your_word_data_fs.pkl\" \n",
    "authors_file = \"your_email_authors_fs.pkl\"\n",
    "word_data = pickle.load( open(words_file, \"r\"))\n",
    "authors = pickle.load( open(authors_file, \"r\") )\n",
    "\n",
    "\n",
    "\n",
    "### test_size is the percentage of events assigned to the test set (the\n",
    "### remainder go into training)\n",
    "### feature matrices changed to dense representations for compatibility with\n",
    "### classifier functions in versions 0.15.2 and earlier\n",
    "from sklearn import model_selection\n",
    "features_train, features_test, labels_train, labels_test = model_selection.train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "features_train = vectorizer.fit_transform(features_train)\n",
    "features_test  = vectorizer.transform(features_test).toarray()\n",
    "\n",
    "\n",
    "### a classic way to overfit is to use a small number\n",
    "### of data points and a large number of features;\n",
    "### train on only 150 events to put ourselves in this regime\n",
    "features_train = features_train[:150].toarray()\n",
    "labels_train   = labels_train[:150]\n",
    "\n",
    "### your code goes here\n",
    "\n",
    "# Importing the Scikit Learn package of Support Vector Machine\n",
    "from sklearn import tree\n",
    "\n",
    "# Importing the Scikit Learn to calcutate the accuracy.\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Importing time to compute the time elapsed.\n",
    "from time import time\n",
    "\n",
    "# Creating the classifier using the linear kernel.\n",
    "clf = tree.DecisionTreeClassifier(min_samples_split = 50)\n",
    "\n",
    "# Saving time to compute the elapse time of fitting process.\n",
    "t0 = time()\n",
    "\n",
    "# Fitting/Training clf based on training dataframes.\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "# Calculating the elapse time of fit calculation.\n",
    "print \"training time:\", round(time()-t0, 3), \"s\"\n",
    "\n",
    "# Saving time to compute the elapse time of predicting process. \n",
    "t1 = time()\n",
    "\n",
    "# Storing the predict from features_test in pred.\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "# Calculating the elapse time of predicting calculation.\n",
    "print \"predict time:\", round(time()-t1, 3), \"s\"\n",
    "\n",
    "# Calculating the accuracy and storing in acc.\n",
    "acc = accuracy_score(pred, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14343</th>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            imp\n",
       "14343  0.666667"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting array in DataFrame.\n",
    "importance = pd.DataFrame(clf.feature_importances_, columns = ['imp'])\n",
    "\n",
    "# Filtering the features greater than 0.2\n",
    "importance[importance.imp > 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cgermannsf'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the word which is very dominant.\n",
    "vectorizer.get_feature_names()[14343].encode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Does another highly powerful word arise after you get rid of the first \"signature word\"?** (Hint: the answer is yes)\n",
    ">\n",
    ">**What is this word?**\n",
    "\n",
    "cgermannsf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8 - Checking Important Features Again <a id='part_i_8'></a>\n",
    "\n",
    "Update vectorize_test.py one more time, and rerun. Then run find_signature.py again. Any other important features (importance>0.2) arise? How many? Do any of them look like “signature words”, or are they more “email content” words, that look like they legitimately come from the text of the messages?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Starter code to process the emails from Sara and Chris to extract\n",
    "    the features and get the documents ready for classification.\n",
    "\n",
    "    The list of all the emails from Sara are in the from_sara list\n",
    "    likewise for emails from Chris (from_chris)\n",
    "\n",
    "    The actual documents are in the Enron email dataset, which\n",
    "    you downloaded/unpacked in Part 0 of the first mini-project. If you have\n",
    "    not obtained the Enron email corpus, run startup.py in the tools folder.\n",
    "\n",
    "    The data is stored in lists and packed away in pickle files at the end.\n",
    "\"\"\"\n",
    "\n",
    "from_sara  = open(\"../text_learning/from_sara.txt\", \"r\")\n",
    "from_chris = open(\"../text_learning/from_chris.txt\", \"r\")\n",
    "\n",
    "from_data = []\n",
    "word_data = []\n",
    "\n",
    "### temp_counter is a way to speed up the development--there are\n",
    "### thousands of emails from Sara and Chris, so running over all of them\n",
    "### can take a long time\n",
    "### temp_counter helps you only look at the first 200 emails in the list so you\n",
    "### can iterate your modifications quicker\n",
    "temp_counter = 0\n",
    "\n",
    "# Start\n",
    "start = time.time()\n",
    "\n",
    "for name, from_person in [(\"sara\", from_sara), (\"chris\", from_chris)]:\n",
    "    for path in from_person:\n",
    "        ### only look at first 200 emails when developing\n",
    "        ### once everything is working, remove this line to run over full dataset\n",
    "        temp_counter += 1\n",
    "        if temp_counter > -1:\n",
    "            path = os.path.join('..', path[:-1])\n",
    "            #print path\n",
    "            email = open(path, \"r\")\n",
    "\n",
    "            ### use parseOutText to extract the text from the opened email\n",
    "            parsed_email = parseOutText_v2(email)\n",
    "                \n",
    "            ### use str.replace() to remove any instances of the words\n",
    "            ### [\"sara\", \"shackleton\", \"chris\", \"germani\"]\n",
    "            parsed_email_clean = parsed_email.replace('sara','').replace('chris','').replace('shackleton','').replace('germani','')\n",
    "            \n",
    "            parsed_email_clean = parsed_email_clean.replace('sshacklensf','').replace('cgermannsf','')\n",
    "            \n",
    "            ### append the text to word_data\n",
    "            word_data.append(parsed_email_clean)\n",
    "            \n",
    "            ### append a 0 to from_data if email is from Sara, and 1 if email is from Chris\n",
    "            if name == 'sara':\n",
    "                from_data.append(0)\n",
    "            elif name == 'chris':\n",
    "                from_data.append(1)\n",
    "            else:\n",
    "                print \"ERROR!!\"\n",
    "            \n",
    "            email.close()\n",
    "\n",
    "print \"emails processed\"\n",
    "from_sara.close()\n",
    "from_chris.close()\n",
    "\n",
    "pickle.dump( word_data, open(\"your_word_data_fs_2.pkl\", \"w\") )\n",
    "pickle.dump( from_data, open(\"your_email_authors_fs_2.pkl\", \"w\") )\n",
    "\n",
    "# Stop\n",
    "stop = time.time()\n",
    "\n",
    "print \"Processing time: \", round((stop - start)/60,2), \"minutes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### in Part 4, do TfIdf vectorization here\n",
    "\n",
    "# Importing the TfIdf package.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Creating the vectorizer using english stopwords.\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english', lowercase=True)\n",
    "\n",
    "# Fitting and Transforming.\n",
    "vectorizer.fit_transform(word_data)\n",
    "\n",
    "# Creating the Vocabulary List.\n",
    "vocab_list = vectorizer.get_feature_names()\n",
    "\n",
    "# Length of the Vocabulary.\n",
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import pickle\n",
    "import numpy\n",
    "numpy.random.seed(42)\n",
    "\n",
    "\n",
    "### The words (features) and authors (labels), already largely processed.\n",
    "### These files should have been created from the previous (Lesson 10)\n",
    "### mini-project.\n",
    "words_file = \"your_word_data_fs_2.pkl\" \n",
    "authors_file = \"your_email_authors_fs_2.pkl\"\n",
    "word_data = pickle.load( open(words_file, \"r\"))\n",
    "authors = pickle.load( open(authors_file, \"r\") )\n",
    "\n",
    "\n",
    "\n",
    "### test_size is the percentage of events assigned to the test set (the\n",
    "### remainder go into training)\n",
    "### feature matrices changed to dense representations for compatibility with\n",
    "### classifier functions in versions 0.15.2 and earlier\n",
    "from sklearn import model_selection\n",
    "features_train, features_test, labels_train, labels_test = model_selection.train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "features_train = vectorizer.fit_transform(features_train)\n",
    "features_test  = vectorizer.transform(features_test).toarray()\n",
    "\n",
    "\n",
    "### a classic way to overfit is to use a small number\n",
    "### of data points and a large number of features;\n",
    "### train on only 150 events to put ourselves in this regime\n",
    "features_train = features_train[:150].toarray()\n",
    "labels_train   = labels_train[:150]\n",
    "\n",
    "### your code goes here\n",
    "\n",
    "# Importing the Scikit Learn package of Support Vector Machine\n",
    "from sklearn import tree\n",
    "\n",
    "# Importing the Scikit Learn to calcutate the accuracy.\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Importing time to compute the time elapsed.\n",
    "from time import time\n",
    "\n",
    "# Creating the classifier using the linear kernel.\n",
    "clf = tree.DecisionTreeClassifier(min_samples_split = 50)\n",
    "\n",
    "# Saving time to compute the elapse time of fitting process.\n",
    "t0 = time()\n",
    "\n",
    "# Fitting/Training clf based on training dataframes.\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "# Calculating the elapse time of fit calculation.\n",
    "print \"training time:\", round(time()-t0, 3), \"s\"\n",
    "\n",
    "# Saving time to compute the elapse time of predicting process. \n",
    "t1 = time()\n",
    "\n",
    "# Storing the predict from features_test in pred.\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "# Calculating the elapse time of predicting calculation.\n",
    "print \"predict time:\", round(time()-t1, 3), \"s\"\n",
    "\n",
    "# Calculating the accuracy and storing in acc.\n",
    "acc = accuracy_score(pred, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting array in DataFrame.\n",
    "importance = pd.DataFrame(clf.feature_importances_, columns = ['imp'])\n",
    "\n",
    "# Filtering the features greater than 0.2\n",
    "importance[importance.imp > 0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the word which is very dominant.\n",
    "vectorizer.get_feature_names()[21323].encode('utf-8'), vectorizer.get_feature_names()[18849].encode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Once you've removed the signature words and reprocess emails, do any \"new important features\" (importance > 0.2) arise? How Much?**\n",
    "\n",
    "houectect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9 - Accuracy of the Overfit Tree <a id='part_i_9'></a>\n",
    "\n",
    "What’s the accuracy of the decision tree now? We've removed two \"signature words\", so it will be more difficult for the algorithm to fit to our limited training set without overfitting. Remember, the whole point was to see if we could get the algorithm to overfit--a sensible result is one where the accuracy isn't that great!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the accuracy.\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**What’s the accuracy of the decision tree now?**\n",
    "\n",
    "81.63%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copying Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing shutil to deal with copy\n",
    "from shutil import copyfile\n",
    "\n",
    "# File name\n",
    "filename = 'find_signature.ipynb'\n",
    "\n",
    "# Lesson\n",
    "lesson = '12-Lesson_12'\n",
    "\n",
    "# Directory to make a copy\n",
    "dir_copy = '../../' + lesson + '/00-Mini Project/' + filename\n",
    "\n",
    "# Copying file.\n",
    "copyfile(filename, dir_copy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
